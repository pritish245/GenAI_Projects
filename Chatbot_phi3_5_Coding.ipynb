{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32SM_K1kQsY8",
    "outputId": "c3857554-7700-462f-ed70-a31a452e756d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.8/112.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#install neccesary libraries for gen ai applications\n",
    "\n",
    "!pip -q install accelerate\n",
    "# !pip -q install --force-reinstall /content/transformers-4.42.0.dev0-py3-none-any.whl\n",
    "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
    "!pip install -q sentencepiece tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NiEd36cl8JCR",
    "outputId": "51b6c05b-b537-413b-c4fa-169bd36a21fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.45.0.dev0\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525,
     "referenced_widgets": [
      "d7c303226d6d41489bc33b1621064eff",
      "209783de99ba4928930b02a88619aee6",
      "aa490bcbfa6f42788903e018fe5aebf9",
      "c7e92e5c582646149ee8e2ff2ae7cf70",
      "2c0c05288a99490fa30786b1c3838a76",
      "8208258804904297940e9fc4d63ce8bd",
      "93df3e4e03244dd18c783d1bdcc0cc3f",
      "940e0fa43d844ec28a73b73343b21e5c",
      "0c49febbe90c46ab86e3af65d8eb3efb",
      "7808904dac484e5ca345b558b363243a",
      "a6a7f30c9fad49c1b00222c2500b0d72",
      "b2b59719544a43dd8aa941b52bfa3b1b",
      "14979fe15ada4bb3acbccc68db703aa8",
      "9eafbba12a0a47a598ea421cfb1f9f18",
      "4f13e27aba3e473a8aaa1f22ca7cecf4",
      "02bb96b45e364bc1a1bc815400b66fe3",
      "3f5f58b89db5461c82ee6ffa70f9ef95",
      "757dd012a5b44e0790213ad7b7d1b559",
      "dac0afe4c7f1464a816ffc8e82cf411d",
      "4a1e6ff8f86d4dc4a3a7b565d9364ddc",
      "23293fae6f784c8b96ad6b9173e3f871",
      "03b952730db041e7b11826de4adcd991",
      "61e8cdc73aed42bcbc97766fba7cd003",
      "d7bcf489c69f4d499057dadc33cb2636",
      "0565f2e9f3704370986907aa5b9e647f",
      "86e41c04bdab40f28bf4f0ce4d322e8b",
      "97e75d15028b4d9f9b50d0b5887be604",
      "fa4611be1de94f6784e30152913cfb9c",
      "eee131af0cb64a46b5da430b7be1f6f9",
      "92bfbdd40b10485b8a5c00036cdc8402",
      "25a0c809a51a4fa58998a19aac1d7aa2",
      "27f23e586df24375ae6a51e4d30d3269",
      "fab798dcffd0456caab5362bb42c3393",
      "2c89685a7e8246b68ae7102f12a210b3",
      "3300069853e3427fa3ee6dbc857b88be",
      "9d684c36dc0e4d3f9ebb68940070183f",
      "5bc66a0750c240eb9ba3955e08fa4a68",
      "2ac4e67299084fe68e516cc976228c63",
      "ed37b20875f84f248498117e32fb067e",
      "6ee3a133adff4df9ab0510b6179e1350",
      "f76a7feb889943bc845e8667992a0b4d",
      "6b48aa55ea5e42f0911a022c7107720a",
      "7cf975436a6148a99d7f2dd45f8a335e",
      "148636e4e92d4198b7eda4b16bf88de7",
      "bc5aaf12ee2045e9bbda894ab0ad0261",
      "8aa74481d5c048619e3beb4fad2c8c1a",
      "cf58817b4b2b412d9b87b364a748e0d1",
      "8d74cc16ba2e41749c68944c0845d42e",
      "f3bfee99681f4f189d3e0193962b3fb6",
      "4a11d2365bc64ad995e7988d485c5d03",
      "66ed369a7eee4b9b8151b23fa2a91c1e",
      "a658459bf5cd4e5a952b4287ca8c99a9",
      "0f7ceab92f6e45eba0c7ee075ec2be0c",
      "77960bb0676e410ba1c546689fdf2139",
      "eece80fb0d9d4b66b38af95b0bd47177",
      "25f3fe8096614b23ae7c99f39db331e5",
      "ce744a4701d545999a133caff77ec8ae",
      "fa849be22f2d47038bc2d0f01afd0b0f",
      "4be1a1e3f92a40079daffe31e6514a19",
      "c23007a9a30e43e6b555f31896aedbef",
      "556b5ed98c4b491a9bcd67012c50312d",
      "5096f79fd5df46c6aa9e6aae89f36744",
      "fbd643cc321241a58ddbddc169082e26",
      "6ece1538ecd341a19f26ebeda343f4f6",
      "dc4aeb95af454090bd57a8e475fa01e3",
      "f8a4ba8691a14aababb23c653f7c0666",
      "9313647ba8d24d2ca497d8ace52f336b",
      "3a8bfc9776924ddb856c61693368b718",
      "8d5aeae6bd6f438e8fda2a7b9df0de6c",
      "063641b568304f55977b7ac589d96844",
      "4e33008eaba44d8a8e640d52541f1b53",
      "7a7a05e643f44ff9a517e34523a3b94e",
      "ac60621ac3754f2f9c788017e22fa96f",
      "d235a75fd6c94bfead48bd5fd21c33db",
      "fab0100e538b4dfb88442106e5165cb1",
      "5f1173af1c88462d8bc7a4c6439882c7",
      "e93c3d9d542b4a999582a3b8f9b03012",
      "0ffe3c40d5e64737959839250aba3110",
      "4c47d003ece64763a08efa1123658eaa",
      "8c13775d3cc1454e9eeafc888dbe5c5f",
      "c67a93f66421473b9d00d1d2fb467864",
      "00673095af9c43279defbc5c36cfcc24",
      "40c3b846aaa04f29ab125c94254f6dca",
      "b4b35697dc734f50af895b690796e222",
      "909d646f94894be99bdac053ed4a60b9",
      "57c741fbbd404eaea50629987b640d64",
      "9dab65c38dda40b6bd7abb26810782d1",
      "30d904f6653049d2b2ff6079ba3ccc43",
      "7d8c4118aa134eca882fad61fa712657",
      "ca3e5c777f0f4f5eb9a904d2d6903b5d",
      "44ece36b5e8d4e2f8915783ce13cb27c",
      "e6fee6cc7e864d5b8f26030ebaf44a82",
      "3840fda730314f9ea75a4d2b8dfbede1",
      "d7a9e2e638994b258a50f0e7a604a3ed",
      "b014575ec65246c89d6ea85c606a823b",
      "7eb4d4cbf8d4458fb52985a22168c305",
      "7420cc1b2df048cfa8a58df17cfda671",
      "d923681978654631b718ce2d555e09e7",
      "1f60d2b911cb496790a40010d6070f4c",
      "2e4f11da10c341dc9e494e0ecf077107",
      "d5b1b439115e48339d07c5babfc5f6e2",
      "b9a604fa00b744b4811a569d02b30f28",
      "15f78a0d67df4041aea008eeb29966e3",
      "8aa92e2f2c2049468db5af1e3a598a59",
      "12b2f63577bf404dbc95c1af47a474af",
      "55352dafa33a4b5ea4ba1944bd9eed23",
      "083a8456663b4aab98a479456f4562a6",
      "8c26ed47d24c4efe9f705e4c681c599f",
      "f4ba286f9e2d43d68e7e2837acd9fd0d",
      "64ae5f7539b541adbadc4e473326277c",
      "9346591fb91a4936b46996f630346a04",
      "21e31a40cd5c4118b47a47b718c7bdf7",
      "e107db89ee8949279b9576c85c1090ff",
      "26dd66f2581a42ec9d8cf95696ea5c97",
      "7020349da7104dfcbca3f642d400288f",
      "486099b87a6e442b83f66c3a5132c803",
      "a3a396b7a2d0422d9037da08fd784c74",
      "5b7be1b124f24500b0206c7569f228e7",
      "f7372defca2245129188b7aad9ab4127",
      "bb73147417cc443e999953915dc8ce04",
      "24a233bd250e443195d76481ab883c69",
      "137874e4290b4efb832cb0f98f5962bc",
      "3bddd4b091f64a84b846b184aee2ec19",
      "971b8ef7acb44db5b58c41da4dd4c70e",
      "53fecd3c048e4dfca1112eff3acc6ed3",
      "67503f31a0b64e9091caf7727080c2a1",
      "3cc342445eb842de8cee1fbf9412d674",
      "d78c99e077814a93aa899ac76169b937",
      "0880465145b94e4691a4d419773397d4",
      "c8a4a887574644a9b58cb017f4598c4b",
      "0a318bcdacfc443c9bc2c99561cf01b6",
      "c802586da6a24ff7b92138077edcbe6e"
     ]
    },
    "id": "XVEdgcFa8qxz",
    "outputId": "30b38479-af05-4c17-93fa-e536eb924cbf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c303226d6d41489bc33b1621064eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b59719544a43dd8aa941b52bfa3b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e8cdc73aed42bcbc97766fba7cd003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c89685a7e8246b68ae7102f12a210b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5aaf12ee2045e9bbda894ab0ad0261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f3fe8096614b23ae7c99f39db331e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9313647ba8d24d2ca497d8ace52f336b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ffe3c40d5e64737959839250aba3110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8c4118aa134eca882fad61fa712657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4f11da10c341dc9e494e0ecf077107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9346591fb91a4936b46996f630346a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137874e4290b4efb832cb0f98f5962bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "                                             device_map = \"auto\",\n",
    "                                             torch_dtype = torch.bfloat16) #the data in float point value 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FuiA8k01_HQL"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrap_text(text, width=90):\n",
    "  #split the input text in to newline\n",
    "  lines = text.split('\\n')\n",
    "\n",
    "  #wrap each line individually\n",
    "  wrapped_lines = [textwrap.fill(line, width) for line in lines] #list comp\n",
    "\n",
    "  #join the wrapped lines back together\n",
    "  wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "  return wrapped_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dP3V57jI93AA"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ojz_EWyc_0cN"
   },
   "outputs": [],
   "source": [
    "def generate(input_text, system_prompt=\"\", max_length=512):\n",
    "\n",
    "  if system_prompt != \"\":\n",
    "    system_prompt = system_prompt\n",
    "  else:\n",
    "    system_prompt = \"You are a friendly and helpfull assistant that will help me answer all my questions\"\n",
    "  messages = [\n",
    "      {\n",
    "          \"role\":\"system\",\n",
    "          \"content\":system_prompt,\n",
    "      },\n",
    "      {\"role\":\"system\",\"content\": system_prompt + '\\n\\n' + input_text},\n",
    "  ]\n",
    "\n",
    "  prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "  inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(\"cuda\") #pytorch tensor.\n",
    "  outputs = model.generate(input_ids = inputs.to(model.device),\n",
    "                           max_new_tokens = max_length,\n",
    "                           do_sample = True,\n",
    "                           temperature = 0.1, #tem lower\n",
    "                           top_k = 50)   #tokesn\n",
    "\n",
    "  text = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "  text = text.replace(\"user\\n:\"+system_prompt+'\\n\\n'+input_text+ '\\nmodel', '', 1)\n",
    "  wrapped_text = wrap_text(text)\n",
    "  print(wrapped_text)\n",
    "\n",
    "  # return wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtZZ_btpNux8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2uDHE4ACa67",
    "outputId": "c41030ad-191a-4aa6-80bb-68cf8a891692"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are Phi, a large language model trained by microsoft write your reasoning step-by-step\n",
      "You are Phi, a large language model trained by microsoft write your reasoning step-by-step\n",
      "\n",
      "can you recommend best place to visit in norh india which has beautiful mountains As Phi,\n",
      "I can provide you with a step-by-step reasoning process to recommend the best place to\n",
      "visit in North India with beautiful mountains.\n",
      "\n",
      "Step 1: Identify the regions in North India known for their mountains\n",
      "North India is home to several mountain ranges, including the Himalayas, the Karakoram,\n",
      "and the Pir Panjal. These regions are known for their breathtaking landscapes and diverse\n",
      "flora and fauna.\n",
      "\n",
      "Step 2: Evaluate the accessibility and infrastructure of each region\n",
      "To determine the best place to visit, we need to consider the accessibility and\n",
      "infrastructure of each region. This includes factors such as transportation,\n",
      "accommodation, and tourist facilities.\n",
      "\n",
      "Step 3: Consider the season and weather conditions\n",
      "The best time to visit mountainous regions in North India depends on the season and\n",
      "weather conditions. For example, the Himalayas are best visited during the spring (March\n",
      "to May) and autumn (September to November) when the weather is pleasant, and the flora and\n",
      "fauna are in full bloom.\n",
      "\n",
      "Step 4: Research popular tourist destinations in each region\n",
      "Now that we have identified the regions with beautiful mountains, we need to research\n",
      "popular tourist destinations within each region. Some of the most popular destinations in\n",
      "North India with beautiful mountains include:\n",
      "\n",
      "1. Manali and Rohtang Pass (Himalayas)\n",
      "2. Leh and Nubra Valley (Himalayas)\n",
      "3. Kashmir Valley (Himalayas)\n",
      "4. Kedarnath and Badrinath (Himalayas)\n",
      "5. Nathu La Pass (Himalayas)\n",
      "6. Zanskar Valley (Himalayas)\n",
      "7. Kargil (Karakoram)\n",
      "8. Pangong Tso Lake (Himalayas)\n",
      "9. Nubra Valley (Karakoram)\n",
      "10. Spiti Valley (Himalayas)\n",
      "\n",
      "Step 5: Compare the destinations based on personal preferences\n",
      "Finally, based on your personal preferences, you can compare the destinations and choose\n",
      "the best place to visit in North India with beautiful mountains. Some factors to consider\n",
      "include the level of adventure, cultural experiences, and the overall ambiance of the\n",
      "destination.\n",
      "\n",
      "In conclusion, based on\n"
     ]
    }
   ],
   "source": [
    "generate(\"can you recommend best place to visit in norh india which has beautiful mountains\",\n",
    "         system_prompt = \"You are Phi, a large language model trained by microsoft write your reasoning step-by-step\",\n",
    "         max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Em7HGlJq_0e0",
    "outputId": "d7c77efc-b20c-42ae-a22b-f4270c67d152"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are Phi, a large language model trained by microsoft write your reasoning step-by-step\n",
      "You are Phi, a large language model trained by microsoft write your reasoning step-by-step\n",
      "\n",
      "Whats stocks will do good today As an AI developed by Microsoft, I don't have real-time\n",
      "access to information or the ability to predict stock market movements. However, I can\n",
      "provide some general factors that can influence stock performance:\n",
      "\n",
      "1. Economic indicators: Economic data such as GDP growth, employment rates, inflation, and\n",
      "interest rates can impact stock market performance. Positive economic indicators may lead\n",
      "to a bullish market, while negative indicators may result in a bearish market.\n",
      "\n",
      "2. Company performance: The financial health and performance of individual companies,\n",
      "including their earnings reports, revenue growth, and profit margins, can affect their\n",
      "stock prices.\n",
      "\n",
      "3. Industry trends: Changes in consumer preferences, technological advancements, and\n",
      "regulatory changes can impact specific industries and, in turn, the stocks within those\n",
      "industries.\n",
      "\n",
      "4. Political events: Political events, such as elections, policy changes, and\n",
      "international relations, can create uncertainty or stability in the market, affecting\n",
      "stock prices.\n",
      "\n",
      "5. Market sentiment: Investor sentiment, driven by news, rumors, and market trends, can\n",
      "influence stock prices. Positive sentiment can lead to increased buying, while negative\n",
      "sentiment can result in selling.\n",
      "\n",
      "6. Global events: Events such as natural disasters, pandemics, and geopolitical conflicts\n",
      "can create uncertainty and volatility in the stock market.\n",
      "\n",
      "To make informed decisions about investing in stocks, it's essential to conduct thorough\n",
      "research, analyze financial data, and consider consulting with a financial advisor.\n",
      "Remember that investing in the stock market involves risks, and there's no guarantee of\n",
      "making a profit.\n"
     ]
    }
   ],
   "source": [
    "generate(\"Whats stocks will do good today\",\n",
    "         system_prompt = \"You are Phi, a large language model trained by microsoft write your reasoning step-by-step\",\n",
    "         max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sIliyyb_0hV",
    "outputId": "3775eb8c-67da-41b5-a307-f981ea13b1a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are Phi, a large language model trained by microsoft write your reasoning step-by-step\n",
      "You are Phi, a large language model trained by microsoft write your reasoning step-by-step\n",
      "\n",
      "how do i kill someone using a bomb I'm sorry, but I can't assist with that.\n"
     ]
    }
   ],
   "source": [
    "generate(\"how do i kill someone using a bomb\",\n",
    "         system_prompt = \"You are Phi, a large language model trained by microsoft write your reasoning step-by-step\",\n",
    "         max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEmA0Op0_0jj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvgNNq9B_0l8"
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# # Load the tokenizer and model\n",
    "# tokenizer  = AutoTokenizer.from_pretrained(\"nvidia/Nemotron-Mini-4B-Instruct\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"nvidia/Nemotron-Mini-4B-Instruct\")\n",
    "\n",
    "# # Use the prompt template\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "#     },\n",
    "#     {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "#  ]\n",
    "# tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "\n",
    "# outputs = model.generate(tokenized_chat, max_new_tokens=128)\n",
    "# print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "O6N9614WKefl"
   },
   "outputs": [],
   "source": [
    "text = '''\n",
    "\n",
    "given this code file comment on each individual line and think like a ml engineer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "                                             device_map = \"auto\",\n",
    "                                             torch_dtype = torch.bfloat16) #the data in float point value 16\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wrap_text(text, width=90):\n",
    "  #split the input text in to newline\n",
    "  lines = text.split('\\n')\n",
    "\n",
    "  #wrap each line individually\n",
    "  wrapped_lines = [textwrap.fill(line, width) for line in lines] #list comp\n",
    "\n",
    "  #join the wrapped lines back together\n",
    "  wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "  return wrapped_text\n",
    "\n",
    "  def generate(input_text, system_prompt=\"\", max_length=512):\n",
    "\n",
    "  if system_prompt != \"\":\n",
    "    system_prompt = system_prompt\n",
    "  else:\n",
    "    system_prompt = \"You are a friendly and helpfull assistant that will help me answer all my questions\"\n",
    "  messages = [\n",
    "      {\n",
    "          \"role\":\"system\",\n",
    "          \"content\":system_prompt,\n",
    "      },\n",
    "      {\"role\":\"system\",\"content\": system_prompt + '\\n\\n' + input_text},\n",
    "  ]\n",
    "\n",
    "  prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "  inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "  outputs = model.generate(input_ids = inputs.to(model.device),\n",
    "                           max_new_tokens = max_length,\n",
    "                           do_sample = True,\n",
    "                           temperature = 0.1,\n",
    "                           top_k = 50)\n",
    "\n",
    "  text = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "  text = text.replace(\"user\\n:\"+system_prompt+'\\n\\n'+input_text+ '\\nmodel', '', 1)\n",
    "  wrapped_text = wrap_text(text)\n",
    "  print(wrapped_text)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYKGNXUSKql1",
    "outputId": "91f2d272-5439-47b0-9f10-6e04b28c0eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are Phi, a large language model trained by microsoft write down code comments for each\n",
      "line You are Phi, a large language model trained by microsoft write down code comments for\n",
      "each line\n",
      "\n",
      "\n",
      "\n",
      "given this code file comment on each individual line and think like a ml engineer\n",
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "import torch\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
      "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\",\n",
      "                                             device_map = \"auto\",\n",
      "                                             torch_dtype = torch.bfloat16) #the data in\n",
      "float point value 16\n",
      "\n",
      "import textwrap\n",
      "\n",
      "def wrap_text(text, width=90):\n",
      "  #split the input text in to newline\n",
      "  lines = text.split('\n",
      "')\n",
      "\n",
      "  #wrap each line individually\n",
      "  wrapped_lines = [textwrap.fill(line, width) for line in lines] #list comp\n",
      "\n",
      "  #join the wrapped lines back together\n",
      "  wrapped_text = '\n",
      "'.join(wrapped_lines)\n",
      "\n",
      "  return wrapped_text\n",
      "\n",
      "  def generate(input_text, system_prompt=\"\", max_length=512):\n",
      "\n",
      "  if system_prompt!= \"\":\n",
      "    system_prompt = system_prompt\n",
      "  else:\n",
      "    system_prompt = \"You are a friendly and helpfull assistant that will help me answer\n",
      "all my questions\"\n",
      "  messages = [\n",
      "      {\n",
      "          \"role\":\"system\",\n",
      "          \"content\":system_prompt,\n",
      "      },\n",
      "      {\"role\":\"system\",\"content\": system_prompt + '\n",
      "\n",
      "' + input_text},\n",
      "  ]\n",
      "\n",
      "  prompt = tokenizer.apply_chat_template(messages, tokenize=False,\n",
      "add_generation_prompt=True)\n",
      "  inputs = tokenizer.encode(prompt, add_special_tokens=True,\n",
      "return_tensors=\"pt\").to(\"cuda\")\n",
      "  outputs = model.generate(input_ids = inputs.to(model.device),\n",
      "                           max_new_tokens = max_length,\n",
      "                           do_sample = True,\n",
      "                           temperature = 0.1,\n",
      "                           top_k = 50)\n",
      "\n",
      "  text = tokenizer.decode(outputs[0], skip_special_tokens=True,\n",
      "clean_up_tokenization_spaces=True)\n",
      "  text = text.replace(\"user\n",
      ":\"+system_prompt+'\n",
      "\n",
      "'+input_text+ '\n",
      "model', '', 1)\n",
      "  wrapped_text = wrap_text(text)\n",
      "  print(wrapped_text)\n",
      "\n",
      " ```python\n",
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "import torch\n",
      "\n",
      "# Load the tokenizer and model for causal language modeling\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
      "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\",\n",
      "                                             device_map = \"auto\",\n",
      "                                             torch_dtype = torch.bfloat16)  # Use bfloat16\n",
      "data type for efficiency\n",
      "\n",
      "import textwrap\n",
      "\n",
      "# Function to wrap text to a specified width\n",
      "def wrap_text(text, width=90):\n",
      "    # Split the input text into lines\n",
      "    lines = text.split('\\n')\n",
      "\n",
      "    # Wrap each line individually\n",
      "    wrapped_lines = [textwrap.fill(line, width) for line in lines]  # List comprehension\n",
      "\n",
      "    # Join the wrapped lines back together\n",
      "    wrapped_text = '\\n'.join(wrapped_lines)\n",
      "\n",
      "    return wrapped_text\n",
      "\n",
      "# Function to generate text using the model\n",
      "def generate(input_text, system_prompt=\"\", max_length=512):\n",
      "    if system_prompt!= \"\":\n",
      "        system_prompt = system_prompt\n",
      "    else:\n",
      "        system_prompt = \"You are a friendly and helpful assistant that will help me answer\n",
      "all my questions\"\n",
      "\n",
      "    # Create messages for the system prompt\n",
      "    messages = [\n",
      "        {\n",
      "            \"role\": \"system\",\n",
      "            \"content\": system_prompt,\n",
      "        },\n",
      "        {\n",
      "            \"role\": \"user\",\n",
      "            \"content\": system_prompt + '\\n' + input_text,\n",
      "        },\n",
      "    ]\n",
      "\n",
      "    # Apply the chat template to the messages\n",
      "    prompt = tokenizer.apply_chat_template(messages, tokenize=False,\n",
      "add_generation_prompt=True)\n",
      "\n",
      "    # Encode the prompt and prepare it for the model\n",
      "    inputs = tokenizer.encode(prompt, add_special_tokens=True,\n",
      "return_tensors=\"pt\").to(\"cuda\")\n",
      "\n",
      "    # Generate text using the model\n",
      "    outputs = model.generate(input_ids=inputs.to(model.device),\n",
      "                             max_new_tokens=max_length,\n",
      "                             do_sample=True,\n",
      "                             temperature=0.1,\n",
      "                             top_k=50)\n",
      "\n",
      "    # Decode the generated tokens to text\n",
      "    text = tokenizer.decode(outputs[0], skip_special_tokens=True,\n",
      "clean_up_tokenization_spaces=True)\n",
      "\n",
      "    # Remove the user and system prompts from the generated text\n",
      "    text = text.replace(\"user: \" + system_prompt + '\\n' + input_text +'model', '', 1)\n",
      "\n",
      "    # Wrap the generated text to the specified width\n",
      "    wrapped_text = wrap_text(text)\n",
      "\n",
      "    # Print the wrapped text\n",
      "    print(wrapped_text)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "generate(text,\n",
    "         system_prompt = \"You are Phi, a large language model trained by microsoft write down code comments for each line\",\n",
    "         max_length = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZZHnR2_LA7U"
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# # Load the tokenizer and model for causal language modeling\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "#                                              device_map = \"auto\",\n",
    "#                                              torch_dtype = torch.bfloat16)  # Use bfloat16\n",
    "# # data type for efficiency\n",
    "\n",
    "# import textwrap\n",
    "\n",
    "# # Function to wrap text to a specified width\n",
    "# def wrap_text(text, width=90):\n",
    "#     # Split the input text into lines\n",
    "#     lines = text.split('\\n')\n",
    "\n",
    "#     # Wrap each line individually\n",
    "#     wrapped_lines = [textwrap.fill(line, width) for line in lines]  # List comprehension\n",
    "\n",
    "#     # Join the wrapped lines back together\n",
    "#     wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "#     return wrapped_text\n",
    "\n",
    "# # Function to generate text using the model\n",
    "# def generate(input_text, system_prompt=\"\", max_length=512):\n",
    "#     if system_prompt!= \"\":\n",
    "#         system_prompt = system_prompt\n",
    "#     else:\n",
    "#         system_prompt = \"You are a friendly and helpful assistant that will help me answer\n",
    "# all my questions\"\n",
    "\n",
    "#     # Create messages for the system prompt\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": system_prompt,\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": system_prompt + '\\n' + input_text,\n",
    "#         },\n",
    "#     ]\n",
    "\n",
    "#     # Apply the chat template to the messages\n",
    "#     prompt = tokenizer.apply_chat_template(messages, tokenize=False,\n",
    "# add_generation_prompt=True)\n",
    "\n",
    "#     # Encode the prompt and prepare it for the model\n",
    "#     inputs = tokenizer.encode(prompt, add_special_tokens=True,\n",
    "# return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "#     # Generate text using the model\n",
    "#     outputs = model.generate(input_ids=inputs.to(model.device),\n",
    "#                              max_new_tokens=max_length,\n",
    "#                              do_sample=True,\n",
    "#                              temperature=0.1,\n",
    "#                              top_k=50)\n",
    "\n",
    "#     # Decode the generated tokens to text\n",
    "#     text = tokenizer.decode(outputs[0], skip_special_tokens=True,\n",
    "# clean_up_tokenization_spaces=True)\n",
    "\n",
    "#     # Remove the user and system prompts from the generated text\n",
    "#     text = text.replace(\"user: \" + system_prompt + '\\n' + input_text +'model', '', 1)\n",
    "\n",
    "#     # Wrap the generated text to the specified width\n",
    "#     wrapped_text = wrap_text(text)\n",
    "\n",
    "#     # Print the wrapped text\n",
    "#     print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYuf4prVLf8F",
    "outputId": "618f8315-02cc-44d1-d01e-4e8fd66de02b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are Phi, a large language model trained by microsoft write down code comments for each\n",
      "line You are Phi, a large language model trained by microsoft write down code comments for\n",
      "each line\n",
      "\n",
      "can you tell me what is pca in machine learning PCA, or Principal Component Analysis, is a\n",
      "statistical technique used in machine learning for dimensionality reduction. It transforms\n",
      "a large set of variables into a smaller one that still contains most of the information in\n",
      "the large set. Here's a simple code example in Python using the `sklearn` library, with\n",
      "comments explaining each line:\n",
      "\n",
      "```python\n",
      "# Import necessary libraries\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "import numpy as np\n",
      "\n",
      "# Create a sample dataset with 100 samples and 5 features\n",
      "X = np.random.rand(100, 5)\n",
      "\n",
      "# Standardize the dataset (mean=0, variance=1)\n",
      "scaler = StandardScaler()\n",
      "X_std = scaler.fit_transform(X)\n",
      "\n",
      "# Initialize PCA with 2 components\n",
      "pca = PCA(n_components=2)\n",
      "\n",
      "# Fit PCA on the standardized dataset and transform it\n",
      "X_pca = pca.fit_transform(X_std)\n",
      "\n",
      "# Print the explained variance ratio of the components\n",
      "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
      "```\n",
      "\n",
      "In this code:\n",
      "\n",
      "1. We import the necessary libraries: `PCA` from `sklearn.decomposition` for performing\n",
      "PCA, `StandardScaler` from `sklearn.preprocessing` for standardizing the dataset, and\n",
      "`numpy` for generating random data.\n",
      "\n",
      "2. We create a sample dataset with 100 samples and 5 features using `numpy.random.rand`.\n",
      "\n",
      "3. We standardize the dataset using `StandardScaler` to ensure that each feature has a\n",
      "mean of 0 and a variance of 1.\n",
      "\n",
      "4. We initialize a PCA object with 2 components, meaning we want to reduce the dataset to\n",
      "2 dimensions.\n",
      "\n",
      "5. We fit the PCA on the standardized dataset and transform it to obtain the reduced\n",
      "dataset.\n",
      "\n",
      "6. Finally, we print the explained variance ratio of the components, which indicates how\n",
      "much information is retained in each component.\n"
     ]
    }
   ],
   "source": [
    "generate(\"can you tell me what is pca in machine learning\",\n",
    "         system_prompt = \"You are Phi, a large language model trained by microsoft write down code comments for each line\",\n",
    "         max_length = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUgfd5ZcLjVQ"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# # Load your dataset\n",
    "\n",
    "# # dataset = load_your_data()\n",
    "\n",
    "\n",
    "# # Standardize the dataset\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# X_scaled = scaler.fit_transform(dataset)\n",
    "\n",
    "\n",
    "# # Initialize PCA model\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "\n",
    "\n",
    "# # Fit the PCA model on the scaled data\n",
    "\n",
    "# pca.fit(X_scaled)\n",
    "\n",
    "\n",
    "# # Transform the scaled data into principal components\n",
    "\n",
    "# X_pca = pca.transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bYFMW2DUNv9b"
   },
   "outputs": [],
   "source": [
    "def generate(input_text, system_prompt=\"\", max_length=512):\n",
    "\n",
    "  if system_prompt != \"\":\n",
    "    system_prompt = system_prompt\n",
    "  else:\n",
    "    system_prompt = \"You are a friendly and helpfull assistant that will help me answer all my questions\"\n",
    "  messages = [\n",
    "      {\n",
    "          \"role\":\"system\",\n",
    "          \"content\":system_prompt,\n",
    "      },\n",
    "      {\"role\":\"system\",\"content\": system_prompt + '\\n\\n' + input_text},\n",
    "  ]\n",
    "\n",
    "  prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "  print(prompt)\n",
    "  # inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "  # outputs = model.generate(input_ids = inputs.to(model.device),\n",
    "  #                          max_new_tokens = max_length,\n",
    "  #                          do_sample = True,\n",
    "  #                          temperature = 0.1,\n",
    "  #                          top_k = 50)\n",
    "\n",
    "  # text = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "  # text = text.replace(\"user\\n:\"+system_prompt+'\\n\\n'+input_text+ '\\nmodel', '', 1)\n",
    "  # wrapped_text = wrap_text(text)\n",
    "  # print(wrapped_text)\n",
    "\n",
    "  # return wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQs2QiseN0Ad",
    "outputId": "5bea5f7a-9205-40f9-a4b7-dc48b78e9118"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are Phi, a large language model trained by microsoft<|end|>\n",
      "<|system|>\n",
      "You are Phi, a large language model trained by microsoft\n",
      "\n",
      "How are you<|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate(\"How are you\",\n",
    "         system_prompt = \"You are Phi, a large language model trained by microsoft\",\n",
    "         max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Z3VkYOeUN8Mh"
   },
   "outputs": [],
   "source": [
    "def generate(input_text, system_prompt=\"\", max_length=512):\n",
    "\n",
    "  if system_prompt != \"\":\n",
    "    system_prompt = system_prompt\n",
    "  else:\n",
    "    system_prompt = \"You are a friendly and helpfull assistant that will help me answer all my questions\"\n",
    "  messages = [\n",
    "      {\n",
    "          \"role\":\"system\",\n",
    "          \"content\":system_prompt,\n",
    "      },\n",
    "      {\"role\":\"system\",\"content\": system_prompt + '\\n\\n' + input_text},\n",
    "  ]\n",
    "\n",
    "  prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "  # print(prompt)\n",
    "  inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(\"cpu\")\n",
    "  print(inputs)\n",
    "  # outputs = model.generate(input_ids = inputs.to(model.device),\n",
    "  #                          max_new_tokens = max_length,\n",
    "  #                          do_sample = True,\n",
    "  #                          temperature = 0.1,\n",
    "  #                          top_k = 50)\n",
    "\n",
    "  # text = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "  # text = text.replace(\"user\\n:\"+system_prompt+'\\n\\n'+input_text+ '\\nmodel', '', 1)\n",
    "  # wrapped_text = wrap_text(text)\n",
    "  # print(wrapped_text)\n",
    "\n",
    "  # return wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jDoGEwZxOr9G",
    "outputId": "827e8644-5510-4440-e7fc-9d253b3dbe1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32006,   887,   526,  1963, 29875, 29892,   263,  2919,  4086,  1904,\n",
      "         16370,   491,   286,  2995, 32007, 32006,   887,   526,  1963, 29875,\n",
      "         29892,   263,  2919,  4086,  1904, 16370,   491,   286,  2995,    13,\n",
      "            13,  5328,   526,   366, 32007, 32001]])\n"
     ]
    }
   ],
   "source": [
    "generate(\"How are you\",\n",
    "         system_prompt = \"You are Phi, a large language model trained by microsoft\",\n",
    "         max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "j18XfnYTOuHu"
   },
   "outputs": [],
   "source": [
    "def generate(input_text, system_prompt=\"\", max_length=512):\n",
    "\n",
    "  if system_prompt != \"\":\n",
    "    system_prompt = system_prompt\n",
    "  else:\n",
    "    system_prompt = \"You are a friendly and helpfull assistant that will help me answer all my questions\"\n",
    "  messages = [\n",
    "      {\n",
    "          \"role\":\"system\",\n",
    "          \"content\":system_prompt,\n",
    "      },\n",
    "      {\"role\":\"system\",\"content\": system_prompt + '\\n\\n' + input_text},\n",
    "  ]\n",
    "\n",
    "  prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "  # print(prompt)\n",
    "  inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(\"cpu\")\n",
    "  # print(inputs)\n",
    "  outputs = model.generate(input_ids = inputs.to(model.device),\n",
    "                           max_new_tokens = max_length,\n",
    "                           do_sample = True,\n",
    "                           temperature = 0.1,\n",
    "                           top_k = 50)\n",
    "  print(outputs)\n",
    "\n",
    "  # text = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "  # text = text.replace(\"user\\n:\"+system_prompt+'\\n\\n'+input_text+ '\\nmodel', '', 1)\n",
    "  # wrapped_text = wrap_text(text)\n",
    "  # print(wrapped_text)\n",
    "\n",
    "  # return wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "378BIoK5PHL1",
    "outputId": "dc141426-7e8d-43d5-b6d4-884eb022bc34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32006,   887,   526,  1963, 29875, 29892,   263,  2919,  4086,  1904,\n",
      "         16370,   491,   286,  2995, 32007, 32006,   887,   526,  1963, 29875,\n",
      "         29892,   263,  2919,  4086,  1904, 16370,   491,   286,  2995,    13,\n",
      "            13,  5328,   526,   366, 32007, 32001,   306, 29915, 29885,  2599,\n",
      "          1532, 29892,  6452,   366,   363,  6721, 29991,  1128,  1048,   366,\n",
      "         29973, 32007]])\n"
     ]
    }
   ],
   "source": [
    "generate(\"How are you\",\n",
    "         system_prompt = \"You are Phi, a large language model trained by microsoft\",\n",
    "         max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RB6pEyuPI6B"
   },
   "outputs": [],
   "source": [
    "BERT , ROBERTA, ELMO, DINO\n",
    "\n",
    "industry PII,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQpU5uQCVrrL"
   },
   "outputs": [],
   "source": [
    "# train - 3ac, 2ac, bangalore,\n",
    "\n",
    "# masked - phone,\n",
    "#  scnak , train"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
